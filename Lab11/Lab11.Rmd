---
title: "Week 11: Splines"
author: "Quynh Vu"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  out.width = "100%",
  fig.width = 10,
  fig.height = 6.7, 
  fig.retina = 3,
  cache = TRUE)
```

```{r echo=FALSE}
library(tidyverse)
library(here)
library(rstan)
library(tidybayes)
source(here("C:/QV/STA 2201/STA2201-W2023/Lab11/getsplines.R"))
library(geofacet)
library(reshape2)
```

# Overview

In this lab you'll be fitting a second-order P-Splines regression model to foster care entries by state in the US, projecting out to 2030. Here's the data

```{r echo=FALSE}
d <- read_csv("https://raw.githubusercontent.com/pquynhvu/STA2201-W2023/main/Lab11/fc_entries.csv", show_col_types = FALSE)
```

## Question 1: Make a plot highlighting trends over time by state. Might be a good opportunity to use `geofacet`. Describe what you see in a couple of sentences. 

```{r echo=FALSE}
d |> ggplot(aes(year, ent_pc)) + 
            geom_line()+
            facet_geo(~state, scales = "free_y")
```
## Question 2: Fit a hierarchical second-order P-Splines regression model to estimate the (logged) entries per capita over the period 2010-2017. The model you want to fit is

$$
\begin{aligned}
y_{st} &\sim N(\log \lambda_{st}, \sigma^2_{y,s})\\
\log \lambda_{st} &= \alpha_kB_k(t)\\
\Delta^2\alpha_k &\sim N(0, \sigma^2_{\alpha,s})\\
\log \sigma_{\alpha,s} &\sim N(\mu_{\sigma}, \tau^2)
\end{aligned}
$$

Where $y_{s,t}$ is the logged entries per capita for state $s$ in year $t$. Use cubic splines that have knots 2.5 years apart and are a constant shape at the boundaries. Put standard normal priors on standard deviations and hyperparameters. 

```{r echo=TRUE}
years <- unique(d$year)
N <- length(years)
y <- log(d |> select(state, year, ent_pc) |> 
              pivot_wider(names_from = "state", values_from = "ent_pc") |> 
              select(-year) |> 
              as.matrix())
res <- getsplines(years, 2.5)
B <- res$B.ik
K <- ncol(B)

stan_data <- list(N = N, y = y, K = K, S = length(unique(d$state)), B = B)
mod <- stan(data = stan_data, file = here("C:/QV/STA 2201/STA2201-W2023/Lab11/lab11.stan"), refresh = 0)
summary(mod)[["summary"]]
```

## Question 3: Project forward entries per capita to 2030. Pick 4 states and plot the results (with 95% CIs). Note the code to do this in R is in the lecture slides. 

```{r echo=FALSE}
state_ix <- which(unique(d$state) %in% c("California", "Mississippi", "Ohio", "Texas"))
dd <- d |> filter(state %in% c("California", "Mississippi", "Ohio", "Texas")) |>
           mutate(state = case_when(state == "California" ~ 'CA', 
                                    state == "Mississippi" ~ 'MS',
                                    state == "Ohio" ~ 'OH',
                                    state == "Texas" ~ 'TX'),
                  ent_pc = ent_pc/1000)|>
           select(year, state, ent_pc)
```

```{r echo=FALSE}
proj_years <- 2018:2030
B.ik_full <- getsplines(c(years, proj_years), I = 1)$B.ik
K <- ncol(res$B.ik)        # number of knots in sample
K_full <- ncol(B.ik_full)  # number of knots over entire period
proj_steps <- K_full - K   # number of projection steps
# get your posterior samples
post_mean <- get_posterior_mean(mod)
alphas <- extract(mod)[["alpha"]]
sigmas <- extract(mod)[["sigma_alpha"]]
sigma_ys <- extract(mod)[["sigma_y"]]
nsims <- nrow(alphas)
```

```{r echo=FALSE}
alphas_proj <- array(NA, c(nsims, proj_steps, length(unique(d$state))))
set.seed(1098)
# project the alphas
for(j in 1:length(unique(d$state))){
  first_next_alpha <- rnorm(n = nsims,
                            mean = 2*alphas[,K,j] - alphas[,K-1,j],
                            sd = sigmas[,j])
  second_next_alpha <- rnorm(n = nsims,
                            mean = 2*first_next_alpha - alphas[,K,j],
                            sd = sigmas[,j])
  alphas_proj[,1,j] <- first_next_alpha
  alphas_proj[,2,j] <- second_next_alpha
  for(i in 3:proj_steps){ #!!! not over years but over knots
    alphas_proj[,i,j] <- rnorm(n = nsims,
    mean = 2*alphas_proj[,i-1,j] - alphas_proj[,i-2,j],
    sd = sigmas[,j])
  }
}
y_proj <- array(NA, c(nsims, length(proj_years), length(unique(d$state))))
for(i in 1:length(proj_years)){ # now over years
  for(j in 1:length(unique(d$state))){
    all_alphas <- cbind(alphas[,,j], alphas_proj[,,j] )
    this_lambda <- all_alphas %*% as.matrix(B.ik_full[length(unique(d$year))+i, ])
    y_proj[,i,j] <- rnorm(n = nsims, mean = this_lambda, sd = sigma_ys[,j])
  }
}
```

```{r echo=FALSE}
y_proj_CA <- y_proj[,, state_ix[1]] |> t() |> as.data.frame() 
y_proj_MS <- y_proj[,, state_ix[2]] |> t() |> as.data.frame() 
y_proj_OH <- y_proj[,, state_ix[3]] |> t() |> as.data.frame() 
y_proj_TX <- y_proj[,, state_ix[4]] |> t() |> as.data.frame() 
state_projection <- data.frame(year = proj_years, 
                               CA = rowMeans(y_proj_CA)/1000, 
                               MS = rowMeans(y_proj_MS)/1000,
                               OH = rowMeans(y_proj_OH)/1000,
                               TX = rowMeans(y_proj_TX)/1000) |> 
                    pivot_longer(CA:TX, names_to = "state", values_to = "ent_pc")
dd <- rbind(dd, state_projection)
```

```{r echo=FALSE}
state_projection_LB <- data.frame(year = proj_years, 
                                  CA = rowMeans(y_proj_CA)/1000 - 1.96*apply(y_proj_CA/1000, 1, sd, na.rm=TRUE), 
                                  MS = rowMeans(y_proj_MS)/1000 - 1.96*apply(y_proj_MS/1000, 1, sd, na.rm=TRUE),
                                  OH = rowMeans(y_proj_OH)/1000 - 1.96*apply(y_proj_OH/1000, 1, sd, na.rm=TRUE),
                                  TX = rowMeans(y_proj_TX)/1000 - 1.96*apply(y_proj_TX/1000, 1, sd, na.rm=TRUE)) |> 
                    pivot_longer(CA:TX, names_to = "state", values_to = "LB_ent_pc")
state_projection_UB <- data.frame(year = proj_years, 
                                  CA = rowMeans(y_proj_CA)/1000 + 1.96*apply(y_proj_CA/1000, 1, sd, na.rm=TRUE), 
                                  MS = rowMeans(y_proj_MS)/1000 + 1.96*apply(y_proj_MS/1000, 1, sd, na.rm=TRUE),
                                  OH = rowMeans(y_proj_OH)/1000 + 1.96*apply(y_proj_OH/1000, 1, sd, na.rm=TRUE),
                                  TX = rowMeans(y_proj_TX)/1000 + 1.96*apply(y_proj_TX/1000, 1, sd, na.rm=TRUE)) |> 
                    pivot_longer(CA:TX, names_to = "state", values_to = "UB_ent_pc")
state_CI <- left_join(state_projection_LB, state_projection_UB, by = join_by(year, state))
state_CI <- data.frame(ent_pc = state_projection$ent_pc, state_CI)
```

```{r echo=FALSE}
options(scipen = 999)
dd |>  ggplot(aes(x = year, y = ent_pc, color = state)) +
             geom_point() +
             geom_smooth(se = FALSE) +
             geom_ribbon(data = state_CI, aes(ymin = LB_ent_pc, ymax = UB_ent_pc, fill = state), alpha = 0.1) +
             labs(title = "Estimated and projected entries per capita second−order P−splines") + 
                  ylab("Entries per capita") +
             theme_minimal()
```    

## Question 4 (bonus) P-Splines are quite useful in structural time series models, when you are using a model of the form 

$$
f(y_t) = \text{systematic part} + \text{time-specific deviations}
$$
where the systematic part is model with a set of covariates for example, and P-splines are used to smooth data-driven deviations over time. Consider adding covariates to the model you ran above. What are some potential issues that may happen in estimation? Can you think of an additional constraint to add to the model that would overcome these issues?

Adding covariates would potentially reduce bias but increase variance.
Since P-spline extrapolates recent trends into the future, which relies on the difference between the last two splines to smooth data-driven deviations over time, resulting in projections that are out of range of the observed sample and even negative. To overcome these issues, we can place non-negative, monotonicity or shape constraints on P-spline smoothings.
